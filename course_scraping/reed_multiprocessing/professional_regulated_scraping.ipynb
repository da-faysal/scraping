{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import time\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Define deaders and date\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"}\n",
    "today = datetime.today().strftime(\"%d_%b_%y\")\n",
    "\n",
    "\n",
    "# This function generates cover page links\n",
    "def generateCoverPageLink(url):\n",
    "    \"\"\"This function generates cover page links from total courses,\n",
    "    url = url to make request in which total course number is found,\n",
    "    return = cover pages links\"\"\"\n",
    "    \n",
    "    # Store cover page links\n",
    "    coverPageLink = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape total course number\n",
    "    totalCourse = int(s.find(\"span\", class_=\"h1\").text.strip().replace(\",\", \"\"))\n",
    "    \n",
    "    # Create stop page\n",
    "    stopPage = int(np.ceil(totalCourse/100))\n",
    "    \n",
    "    # Iterate through stop page and create cover page links\n",
    "    for page in range(1, stopPage+1):\n",
    "            coverPageLink.append(url + f\"?pageno={page}&sortby=MostPopular&pagesize=100\")\n",
    "    return coverPageLink\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function scrapes individual course links from every cover page links\n",
    "def scrapeIndividualCourseLink(url):\n",
    "    \"\"\"Scrapes individual course link from cover page links,\n",
    "    url = cover page link,\n",
    "    return = Individual course links\"\"\"\n",
    "    \n",
    "    # Store course links\n",
    "    courseLink = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape course links and store\n",
    "    for lnk in s.find_all(\"div\", class_=\"course-overview\"):\n",
    "        courseLink.append(\"https://www.reed.co.uk\" + lnk.find(\"h2\").find(\"a\").get(\"href\"))\n",
    "    return courseLink\n",
    "\n",
    "\n",
    "\n",
    "# This function scrapes individual course info from individual course link\n",
    "def scrapeCourseInfo(url):\n",
    "    \"\"\"url = individual course link,\n",
    "    return = scraped course info as a dataframe\"\"\"\n",
    "    \n",
    "    # Initialize empty list of variables to be scraped\n",
    "    courseTitle = []\n",
    "    subtitle = []\n",
    "    offerPrice = []\n",
    "    originalPrice = []\n",
    "    courseProvider = []\n",
    "    unitSold = []\n",
    "    category = []\n",
    "    savings = []\n",
    "    haveCpd = []\n",
    "    awardingBody = []\n",
    "    qualName = []\n",
    "    cpdPoint = []\n",
    "    isRegulated = []\n",
    "    hasProfCert = []\n",
    "    soldOrEnq = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape course name\n",
    "    try:\n",
    "        courseTitle.append(s.find(\"div\", class_=\"course-title\").find(\"h1\").text.strip())\n",
    "    except:\n",
    "        courseTitle.append(\"na\")\n",
    "        \n",
    "    # Scrape subtitle\n",
    "    try:\n",
    "        subtitle.append(s.find(\"div\", class_=\"course-title\").find(\"h2\").text.strip())\n",
    "    except:\n",
    "        subtitle.append(\"na\")\n",
    "        \n",
    "    # Scrape offer price\n",
    "    try:\n",
    "        offerPrice.append(s.find(\"span\", class_=\"current-price\").text.strip())\n",
    "    except:\n",
    "        offerPrice.append(\"na\")\n",
    "        \n",
    "    # Scrape original price\n",
    "    try:\n",
    "        originalPrice.append(s.find(\"small\", class_=\"vat-status\").text.strip())\n",
    "    except:\n",
    "        originalPrice.append(\"na\")\n",
    "        \n",
    "    # Scrape course provider\n",
    "    try:\n",
    "        try:\n",
    "            courseProvider.append(s.find(\"a\", class_=\"provider-link\").text.strip())\n",
    "        except:\n",
    "            courseProvider.append(s.find(\"span\", class_=\"thumbnail\").text.strip())\n",
    "    except:\n",
    "        courseProvider.append(\"na\")\n",
    "        \n",
    "    # Scrape unit sale\n",
    "    try:\n",
    "        unitSold.append(s.find(id=\"number-enquiries-purchases\").text.strip())\n",
    "    except:\n",
    "        unitSold.append(0)\n",
    "        \n",
    "    # Scrape category\n",
    "    try:\n",
    "        # Scrape total category\n",
    "        totalCat = len(s.find_all(\"ol\", class_=\"breadcrumb pb-0\"))\n",
    "        for cat in range(totalCat):\n",
    "            category.append([x.text.strip() for x in s.find_all(\"ol\", class_=\"breadcrumb pb-0\")[cat].find_all(\"li\")])\n",
    "    except:\n",
    "        category.append(\"na\")\n",
    "        \n",
    "    # Scrape savings\n",
    "    try:\n",
    "        savings.append(s.find(\"span\", class_=\"icon-savings-tag price-saving\").text.strip())\n",
    "    except:\n",
    "        savings.append(\"na\")\n",
    "        \n",
    "    # Does the course have CPD?\n",
    "    try:\n",
    "        haveCpd.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-cpd mt-2\") else 0)\n",
    "    except:\n",
    "        haveCpd.append(0)\n",
    "    \n",
    "    # Scrape awarding body\n",
    "    try:\n",
    "        try:\n",
    "            # Executes if the course is \"Endorsed by\"\n",
    "            awardingBody.append(s.find(\"div\", class_=\"col\").find(\"a\").text.strip())\n",
    "        except:\n",
    "            # Executes if the course is \"Awarded by\"\n",
    "            awardingBody.append(s.find(\"div\", class_=\"small\").find(\"div\").find(\"a\").text.strip())\n",
    "    except:\n",
    "        awardingBody.append(\"na\")\n",
    "            \n",
    "    # Scrape qualification name\n",
    "    try:\n",
    "        qualName.append(s.find(\"div\", class_=\"small\").find(\"h3\", class_=\"h4\").text.strip())\n",
    "    except:\n",
    "        qualName.append(\"na\")\n",
    "        \n",
    "    # Scrape cpd point\n",
    "    try:\n",
    "        cpdPoint.append(s.body.find_all(text=re.compile(\"\\d{1,3}\\sCPD hours / points\"))[0].strip())\n",
    "    except:\n",
    "        cpdPoint.append(0)\n",
    "        \n",
    "    # Is the course regulated? Assign 1 if regulated, otherwise 0\n",
    "    try:\n",
    "        isRegulated.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-regulated mt-2\") else 0)\n",
    "    except:\n",
    "        isRegulated.append(0)\n",
    "        \n",
    "    # Does the course offer professional certification?\n",
    "    try:\n",
    "        hasProfCert.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-professional mt-2\") else 0)\n",
    "    except:\n",
    "        hasProfCert.append(0)\n",
    "    \n",
    "    # Check if the course is sold or enquired. 2 if the course has both purchade and enquired mode\n",
    "    try:\n",
    "        soldOrEnq.append(2 if (s.find(id=\"addToBasket\") and s.find(id=\"enquireNow\"))\\\n",
    "        else 1 if s.find(id=\"addToBasket\") else 0)\n",
    "    except:\n",
    "        soldOrEnq.append(\"na\")\n",
    "        \n",
    "    # Create a df off scraped variables\n",
    "    df = pd.DataFrame({\n",
    "        \"courseTitle\":courseTitle,\n",
    "        \"courseLink\":url,\n",
    "        \"subtitle\":subtitle,\n",
    "        \"courseProvider\":courseProvider,\n",
    "        \"offerPrice\":offerPrice,\n",
    "        \"originalPrice\":originalPrice,\n",
    "        \"unitSold\":unitSold,\n",
    "        \"category\":[category], # This one is not scalar, converting into 1d\n",
    "        \"haveCpd\":haveCpd,\n",
    "        \"cpdPoint\":cpdPoint,\n",
    "        \"awardingBody\":[awardingBody],\n",
    "        \"qualName\":qualName,\n",
    "        \"isRegulated\":isRegulated,\n",
    "        \"hasProfCert\":hasProfCert,\n",
    "        \"savings\":savings,\n",
    "        \"soldOrEnq\":soldOrEnq\n",
    "    })\n",
    "    df = df.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# This function cleans scraped data\n",
    "def cleanAndExtractFeature(df):\n",
    "    \"\"\"df = dataFrame to clean,\n",
    "    return = final cleaned data\"\"\"\n",
    "    \n",
    "    # Copy the input data\n",
    "    finalDf = df.copy()\n",
    "    \n",
    "    # Create course id and insert to the df\n",
    "    finalDf.insert(loc=0, value=finalDf.courseLink.str.split(\"/\").str.get(5).str.replace(\"#\",\"\"), \n",
    "                     column=\"courseId\")\n",
    "    \n",
    "    # Insert date\n",
    "    finalDf.insert(loc=0, value=today, column=\"date\")\n",
    "    \n",
    "    # Clean unit sold\n",
    "    finalDf.unitSold = finalDf.unitSold.apply(lambda x: re.findall(r\"\\d+\", x)).str.join(\"\")\n",
    "    finalDf.unitSold = pd.to_numeric(finalDf.unitSold, errors=\"coerce\").fillna(0).astype(\"int\")\n",
    "    \n",
    "    # Clean saving percent\n",
    "    finalDf[\"savingsPercent\"] = finalDf.savings.str.split(\"Save\").str[-1].str.replace(\"%\", \"\")\n",
    "    finalDf.savingsPercent = pd.to_numeric(finalDf.savingsPercent, errors=\"coerce\").fillna(0).astype(\"int\")\n",
    "    \n",
    "    # Clean offer price\n",
    "    finalDf.offerPrice = finalDf.offerPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\")\n",
    "    \n",
    "    # Clean original price\n",
    "    finalDf.originalPrice = finalDf.originalPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\").str.replace(\")\", \"\")\n",
    "    finalDf.originalPrice = pd.to_numeric(finalDf.originalPrice, errors=\"coerce\")\n",
    "    \n",
    "    # If savings is 0, make offer price equals to original price\n",
    "    finalDf.originalPrice = np.where(finalDf.savingsPercent==0,\n",
    "                                     finalDf.originalPrice.fillna(finalDf.offerPrice), finalDf.originalPrice)\n",
    "    \n",
    "    # Extract CPD point\n",
    "    finalDf[\"cpdPoint\"] = pd.to_numeric(finalDf.cpdPoint.str.join(\"\").str.split(\"CPD\").str[0], errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    # Clean CPD provider\n",
    "    finalDf[\"cpdProvider\"]  = finalDf.cpdProvider.str.join(\"\").str.replace(\"Accredited by\", \"\").str.strip()\n",
    "    \n",
    "    # Remove '\"' from category\n",
    "    finalDf.category = finalDf.category.apply(lambda x: eval(x))\n",
    "    \n",
    "    # Create broadCategory1 from category\n",
    "    finalDf[\"broadCategory1\"] = finalDf.category.str[0].str[0]\n",
    "    \n",
    "    # Create broadCategory2 from category\n",
    "    finalDf[\"broadCategory2\"] = finalDf.category.str[-1].str[0]\n",
    "    \n",
    "    # Create subCategory1 from category\n",
    "    finalDf[\"subCategory1\"] = finalDf.category.str[0].str[-1]\n",
    "    \n",
    "    # Create subCategory1 from category\n",
    "    finalDf[\"subCategory2\"] = finalDf.category.str[-1].str[-1]\n",
    "    \n",
    "    # Extract qualification name\n",
    "    finalDf[\"qualName\"] = np.where(finalDf.qualName.str.contains(\"CPD\"), \"na\", finalDf.qualName)\n",
    "    \n",
    "    # Clean awarding body\n",
    "    finalDf.awardingBody = finalDf.awardingBody.str[1:-1].str[1:-1]\\\n",
    "    .str.strip().replace(r\"^\\s*$\", np.nan, regex=True).fillna(\"na\")\n",
    "    \n",
    "    # Drop \"savings\"\n",
    "    finalDf.drop(\"savings\", axis=1, inplace=True)\n",
    "    \n",
    "    # Drop duplicates by \"courseId\"\n",
    "    finalDf = finalDf.drop_duplicates(\"courseId\")\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all the functions inside main\n",
    "def main(url):\n",
    "    \"\"\"url = url to make the 1st requests to generate cover pages,\n",
    "    return = final cleaned dataframe\"\"\"\n",
    "    \n",
    "    # Record start time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Generates cover pages links\n",
    "    coverPageLink = generateCoverPageLink(url)\n",
    "    \n",
    "    # Store individual course links\n",
    "    courseLink = []\n",
    "    \n",
    "    # This loop ensures maximum no of course links scraped\n",
    "    for _ in range(4):\n",
    "        with ProcessPoolExecutor(max_workers=6) as ex:\n",
    "            # Scrape individual course links\n",
    "            indCourseLink = list(ex.map(scrapeIndividualCourseLink, coverPageLink))\n",
    "            indCourseLink = list(chain(*indCourseLink)) # Flattening the list\n",
    "        courseLink.append(indCourseLink)\n",
    "    courseLink = list(chain(*courseLink))\n",
    "    \n",
    "    # Create a series to drop duplicates by ids. This portion keeps only the unique links\n",
    "    tempSeries = pd.Series(courseLink, name=\"tempLink\")\n",
    "    splitTempSeries = tempSeries.astype(\"str\").str.split(\"/\", expand=True)\n",
    "    splitTempSeries.columns = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"]\n",
    "    duplicatesDropped = splitTempSeries.drop_duplicates(\"f\", keep=\"first\").reset_index(drop=True)\n",
    "    courseLink = duplicatesDropped.agg(\"/\".join, axis=1) # This returns a series of course links\n",
    "    \n",
    "    # Scrapes course info from course link\n",
    "    with ProcessPoolExecutor(max_workers=6) as ex:\n",
    "        courseInfo = pd.concat(list(ex.map(scrapeCourseInfo, courseLink))).reset_index(drop=True)\n",
    "    \n",
    "    # Cleans and engineers new features from the scraped dataframe and returns the final dataframe   \n",
    "    finalDf = cleanAndExtractFeature(courseInfo).reset_index(drop=True)\n",
    "    \n",
    "    # Measure execution time and return the final df\n",
    "    endTime = time.time()\n",
    "    durationInMins = round((endTime-startTime)/60, 2)\n",
    "    print(f\"{url.split('/')[4].capitalize()} ==> {len(courseLink)} Records ==> {durationInMins} Minutes\")\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n    chunked=chunked,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\n    self._validate_conn(conn)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\n    conn.connect()\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 421, in connect\n    tls_in_tls=tls_in_tls,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 429, in ssl_wrap_socket\n    sock, context, tls_in_tls, server_hostname=server_hostname\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 472, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 423, in wrap_socket\n    session=session\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 870, in _create\n    self.do_handshake()\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 1139, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n    timeout=timeout\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 531, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\", line 734, in reraise\n    raise value.with_traceback(tb)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n    chunked=chunked,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\n    self._validate_conn(conn)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\n    conn.connect()\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 421, in connect\n    tls_in_tls=tls_in_tls,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 429, in ssl_wrap_socket\n    sock, context, tls_in_tls, server_hostname=server_hostname\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 472, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 423, in wrap_socket\n    session=session\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 870, in _create\n    self.do_handshake()\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 1139, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/faysal/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 239, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 198, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/home/faysal/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 198, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"<ipython-input-2-eb78b05ccb81>\", line 55, in scrapeCourseInfo\n    r = requests.get(url, headers=HEADERS, verify=False)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 76, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 655, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 498, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a88ed1485335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Scrape professional and regulated courses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregulatedAndProfessionalCourse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.reed.co.uk/courses/regulated/professional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f11820c1bdb0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Scrapes course info from course link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcourseInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapeCourseInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourseLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Cleans and engineers new features from the scraped dataframe and returns the final dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \"\"\"\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "# Scrape professional and regulated courses\n",
    "regulatedAndProfessionalCourse = main(\"https://www.reed.co.uk/courses/regulated/professional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "regulatedAndProfessionalCourse.to_csv(f\"/home/faysal/Desktop/masterData/allProfessionalAndRegulatedCourse/{today}_allRegulatedAndProfessionalCourse.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back the data\n",
    "regulatedAndProfessionalCourse = pd.read_csv(f\"/home/faysal/Desktop/masterData/allProfessionalAndRegulatedCourse/{today}_allRegulatedAndProfessionalCourse.csv\")\n",
    "regulatedAndProfessionalCourse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count no of records scraped\n",
    "regulatedAndProfessionalCourse.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cehck for \"na\"\n",
    "regulatedAndProfessionalCourse[regulatedAndProfessionalCourse.courseProvider==\"na\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
