{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import chain\n",
    "import time\n",
    "import urllib3\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Define deaders and date\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"}\n",
    "today = pd.to_datetime(\"today\").strftime(\"%d_%b_%y\")\n",
    "\n",
    "\n",
    "# This function generates cover page links\n",
    "def generateCoverPageLink(url):\n",
    "    \"\"\"This function generates cover page links from total courses,\n",
    "    url = url to make request in which total course number is found,\n",
    "    return = cover pages links\"\"\"\n",
    "    \n",
    "    # Store cover page links\n",
    "    coverPageLink = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape total course number\n",
    "    totalCourse = int(s.find(\"span\", class_=\"h1\").text.strip().replace(\",\", \"\"))\n",
    "    \n",
    "    # Create stop page\n",
    "    stopPage = int(np.ceil(totalCourse/100))\n",
    "    \n",
    "    # Iterate through stop page and create cover page links\n",
    "    for page in range(1, stopPage+1):\n",
    "            coverPageLink.append(url + f\"?pageno={page}&sortby=MostPopular&pagesize=100\")\n",
    "    return coverPageLink\n",
    "\n",
    "\n",
    "\n",
    "# This function scrapes individual course links from every cover page links\n",
    "def scrapeIndividualCourseLink(url):\n",
    "    \"\"\"Scrapes individual course link from cover page links,\n",
    "    url = cover page link,\n",
    "    return = Individual course links\"\"\"\n",
    "    \n",
    "    # Store course links\n",
    "    courseLink = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape course links and store\n",
    "    for lnk in s.find_all(\"div\", class_=\"course-overview\"):\n",
    "        courseLink.append(\"https://www.reed.co.uk\" + lnk.find(\"h2\").find(\"a\").get(\"href\"))\n",
    "    return courseLink\n",
    "\n",
    "\n",
    "\n",
    "# This function scrapes individual course info from individual course link\n",
    "def scrapeCourseInfo(url):\n",
    "    \"\"\"url = individual course link,\n",
    "    return = scraped course info as a dataframe\"\"\"\n",
    "    \n",
    "    # Initialize empty list of variables to be scraped\n",
    "    courseTitle = []\n",
    "    subtitle = []\n",
    "    offerPrice = []\n",
    "    originalPrice = []\n",
    "    courseProvider = []\n",
    "    unitSold = []\n",
    "    category = []\n",
    "    savings = []\n",
    "    haveCpd = []\n",
    "    awardingBody = []\n",
    "    qualName = []\n",
    "    cpdPoint = []\n",
    "    isRegulated = []\n",
    "    hasProfCert = []\n",
    "    soldOrEnq = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape course name\n",
    "    try:\n",
    "        courseTitle.append(s.find(\"div\", class_=\"course-title\").find(\"h1\").text.strip())\n",
    "    except:\n",
    "        courseTitle.append(\"na\")\n",
    "        \n",
    "    # Scrape subtitle\n",
    "    try:\n",
    "        subtitle.append(s.find(\"div\", class_=\"course-title\").find(\"h2\").text.strip())\n",
    "    except:\n",
    "        subtitle.append(\"na\")\n",
    "        \n",
    "    # Scrape offer price\n",
    "    try:\n",
    "        offerPrice.append(s.find(\"span\", class_=\"current-price\").text.strip())\n",
    "    except:\n",
    "        offerPrice.append(\"na\")\n",
    "        \n",
    "    # Scrape original price\n",
    "    try:\n",
    "        originalPrice.append(s.find(\"small\", class_=\"vat-status\").text.strip())\n",
    "    except:\n",
    "        originalPrice.append(\"na\")\n",
    "        \n",
    "    # Scrape course provider\n",
    "    try:\n",
    "        try:\n",
    "            # If the provider is hyperlinked\n",
    "            courseProvider.append(s.find(\"a\", class_=\"provider-link\").text.strip())\n",
    "        except:\n",
    "            # If the provider is not hyperlinked\n",
    "            courseProvider.append(s.find(\"span\", class_=\"thumbnail\").text.strip())\n",
    "    except:\n",
    "        courseProvider.append(\"na\")\n",
    "        \n",
    "    # Scrape unit sale\n",
    "    try:\n",
    "        unitSold.append(s.find(id=\"number-enquiries-purchases\").text.strip())\n",
    "    except:\n",
    "        unitSold.append(0)\n",
    "        \n",
    "    # Scrape category\n",
    "    try:\n",
    "        # Scrape total category\n",
    "        totalCat = len(s.find_all(\"ol\", class_=\"breadcrumb pb-0\"))\n",
    "        for cat in range(totalCat):\n",
    "            category.append([x.text.strip() for x in s.find_all(\"ol\", class_=\"breadcrumb pb-0\")[cat].find_all(\"li\")])\n",
    "    except:\n",
    "        category.append(\"na\")\n",
    "        \n",
    "    # Scrape savings\n",
    "    try:\n",
    "        savings.append(s.find(\"span\", class_=\"icon-savings-tag price-saving\").text.strip())\n",
    "    except:\n",
    "        savings.append(\"na\")\n",
    "        \n",
    "    # Does the course have CPD?\n",
    "    try:\n",
    "        haveCpd.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-cpd mt-2\") else 0)\n",
    "    except:\n",
    "        haveCpd.append(0)\n",
    "    \n",
    "    # Scrape awarding body\n",
    "    try:\n",
    "        try:\n",
    "            # Executes if the course is \"Endorsed by\"\n",
    "            awardingBody.append(s.find(\"div\", class_=\"col\").find(\"a\").text.strip())\n",
    "        except:\n",
    "            # Executes if the course is \"Awarded by\"\n",
    "            awardingBody.append(s.find(\"div\", class_=\"small\").find(\"div\").find(\"a\").text.strip())\n",
    "    except:\n",
    "        awardingBody.append(\"na\")\n",
    "            \n",
    "    # Scrape qualification name, only for awarded courses\n",
    "    try:\n",
    "        qualName.append(s.find(\"div\", class_=\"small\").find(\"h3\", class_=\"h4\").text.strip())\n",
    "    except:\n",
    "        qualName.append(\"na\")\n",
    "        \n",
    "    # Scrape cpd point\n",
    "    try:\n",
    "        cpdPoint.append(s.body.find_all(text=re.compile(\"\\d{1,3}\\sCPD hours / points\"))[0].strip())\n",
    "    except:\n",
    "        cpdPoint.append(0)\n",
    "        \n",
    "    # Is the course regulated? Assign 1 if regulated, otherwise 0\n",
    "    try:\n",
    "        isRegulated.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-regulated mt-2\") else 0)\n",
    "    except:\n",
    "        isRegulated.append(0)\n",
    "        \n",
    "    # Does the course offer professional certification?\n",
    "    try:\n",
    "        hasProfCert.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-professional mt-2\") else 0)\n",
    "    except:\n",
    "        hasProfCert.append(0)\n",
    "    \n",
    "    # Check if the course is sold or enquired. 2 if the course has both purchased and enquired mode\n",
    "    try:\n",
    "        soldOrEnq.append(2 if (s.find(id=\"addToBasket\") and s.find(id=\"enquireNow\"))\\\n",
    "        else 1 if s.find(id=\"addToBasket\") else 0)\n",
    "    except:\n",
    "        soldOrEnq.append(\"na\")\n",
    "        \n",
    "    # Create a df off scraped variables\n",
    "    df = pd.DataFrame({\n",
    "        \"courseTitle\":courseTitle,\n",
    "        \"courseLink\":url,\n",
    "        \"subtitle\":subtitle,\n",
    "        \"courseProvider\":courseProvider,\n",
    "        \"offerPrice\":offerPrice,\n",
    "        \"originalPrice\":originalPrice,\n",
    "        \"unitSold\":unitSold,\n",
    "        \"category\":[category], # This one is not scalar, converting into 1d\n",
    "        \"haveCpd\":haveCpd,\n",
    "        \"cpdPoint\":cpdPoint,\n",
    "        \"awardingBody\":[awardingBody],\n",
    "        \"qualName\":qualName,\n",
    "        \"isRegulated\":isRegulated,\n",
    "        \"hasProfCert\":hasProfCert,\n",
    "        \"savings\":savings,\n",
    "        \"soldOrEnq\":soldOrEnq\n",
    "    })\n",
    "    df = df.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# This function cleans scraped data\n",
    "def cleanAndExtractFeature(df):\n",
    "    \"\"\"df = dataFrame to clean,\n",
    "    return = final cleaned data\"\"\"\n",
    "    \n",
    "    # Copy the input data\n",
    "    finalDf = df.copy()\n",
    "    \n",
    "    # Create course id and insert to the df\n",
    "    finalDf.insert(loc=0, value=finalDf.courseLink.str.split(\"/\").str.get(5).str.replace(\"#\",\"\"), \n",
    "                     column=\"courseId\")\n",
    "    \n",
    "    # Insert date\n",
    "    finalDf.insert(loc=0, value=today, column=\"date\")\n",
    "    \n",
    "    # Clean unit sold\n",
    "    finalDf.unitSold = finalDf.unitSold.apply(lambda x: re.findall(r\"\\d+\", x)).str.join(\"\")\n",
    "    finalDf.unitSold = pd.to_numeric(finalDf.unitSold, errors=\"coerce\").fillna(0).astype(\"int\")\n",
    "    \n",
    "    # Clean saving percent\n",
    "    finalDf[\"savingsPercent\"] = finalDf.savings.str.split(\"Save\").str[-1].str.replace(\"%\", \"\")\n",
    "    finalDf.savingsPercent = pd.to_numeric(finalDf.savingsPercent, errors=\"coerce\").fillna(0).astype(\"int\")\n",
    "    \n",
    "    # Clean offer price\n",
    "    finalDf.offerPrice = finalDf.offerPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\")\n",
    "    \n",
    "    # Clean original price\n",
    "    finalDf.originalPrice = finalDf.originalPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\").str.replace(\")\", \"\")\n",
    "    finalDf.originalPrice = pd.to_numeric(finalDf.originalPrice, errors=\"coerce\")\n",
    "    \n",
    "    # If savings is 0, make offer price equals to original price\n",
    "    finalDf.originalPrice = np.where(finalDf.savingsPercent==0,\n",
    "                                     finalDf.originalPrice.fillna(finalDf.offerPrice), finalDf.originalPrice)\n",
    "    \n",
    "    # Extract CPD point\n",
    "    finalDf[\"cpdPoint\"] = pd.to_numeric(finalDf.cpdPoint.str.join(\"\").str.split(\"CPD\").str[0], errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    # Clean CPD provider\n",
    "    finalDf[\"cpdProvider\"]  = finalDf.cpdProvider.str.join(\"\").str.replace(\"Accredited by\", \"\").str.strip()\n",
    "    \n",
    "    # Remove '\"' from category\n",
    "    finalDf.category = finalDf.category.apply(lambda x: eval(x))\n",
    "    \n",
    "    # Create broadCategory1 from category\n",
    "    finalDf[\"broadCategory1\"] = finalDf.category.str[0].str[0]\n",
    "    \n",
    "    # Create broadCategory2 from category\n",
    "    finalDf[\"broadCategory2\"] = finalDf.category.str[-1].str[0]\n",
    "    \n",
    "    # Create subCategory1 from category\n",
    "    finalDf[\"subCategory1\"] = finalDf.category.str[0].str[-1]\n",
    "    \n",
    "    # Create subCategory1 from category\n",
    "    finalDf[\"subCategory2\"] = finalDf.category.str[-1].str[-1]\n",
    "    \n",
    "    # Extract qualification name\n",
    "    finalDf[\"qualName\"] = np.where(finalDf.qualName.str.contains(\"CPD\"), \"na\", finalDf.qualName)\n",
    "    \n",
    "    # Clean awarding body\n",
    "    finalDf.awardingBody = finalDf.awardingBody.str[1:-1].str[1:-1]\\\n",
    "    .str.strip().replace(r\"^\\s*$\", np.nan, regex=True).fillna(\"na\")\n",
    "    \n",
    "    # Drop \"savings\"\n",
    "    finalDf.drop(\"savings\", axis=1, inplace=True)\n",
    "    \n",
    "    # Drop duplicates by \"courseId\"\n",
    "    finalDf = finalDf.drop_duplicates(\"courseId\")\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', OSError(0, 'Error'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n    chunked=chunked,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\n    self._validate_conn(conn)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\n    conn.connect()\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 421, in connect\n    tls_in_tls=tls_in_tls,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 429, in ssl_wrap_socket\n    sock, context, tls_in_tls, server_hostname=server_hostname\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 472, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 423, in wrap_socket\n    session=session\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 870, in _create\n    self.do_handshake()\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 1139, in do_handshake\n    self._sslobj.do_handshake()\nOSError: [Errno 0] Error\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n    timeout=timeout\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 531, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/packages/six.py\", line 734, in reraise\n    raise value.with_traceback(tb)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n    chunked=chunked,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\n    self._validate_conn(conn)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\n    conn.connect()\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\", line 421, in connect\n    tls_in_tls=tls_in_tls,\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 429, in ssl_wrap_socket\n    sock, context, tls_in_tls, server_hostname=server_hostname\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 472, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 423, in wrap_socket\n    session=session\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 870, in _create\n    self.do_handshake()\n  File \"/home/faysal/anaconda3/lib/python3.7/ssl.py\", line 1139, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', OSError(0, 'Error'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/faysal/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 239, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 198, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/home/faysal/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 198, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"<ipython-input-2-709f9b83ccc0>\", line 40, in scrapeIndividualCourseLink\n    r = requests.get(url, headers=HEADERS, verify=False)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 76, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 655, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/faysal/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 498, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', OSError(0, 'Error'))\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mmain1\u001b[0;34m(url)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \"\"\"\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', OSError(0, 'Error'))"
     ]
    }
   ],
   "source": [
    "# Wrap all the functions inside main\n",
    "def main1(url):\n",
    "    \"\"\"url = url to make the 1st requests to generate cover pages,\n",
    "    return = final cleaned dataframe\"\"\"\n",
    "    \n",
    "    # Record start time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Generates cover pages links\n",
    "    coverPageLink = generateCoverPageLink(url)\n",
    "    \n",
    "    # Store individual course links\n",
    "    courseLink = []\n",
    "    \n",
    "    # This loop ensures maximum no of course links scraped\n",
    "    for _ in range(6):\n",
    "        with ProcessPoolExecutor(max_workers=6) as ex:\n",
    "            # Scrape individual course links\n",
    "            indCourseLink = list(ex.map(scrapeIndividualCourseLink, coverPageLink))\n",
    "            indCourseLink = list(chain(*indCourseLink)) # Flattening the list\n",
    "        courseLink.append(indCourseLink)\n",
    "    courseLink = list(chain(*courseLink))\n",
    "    \n",
    "    # Create a series to drop duplicates by ids. This portion keeps only the unique links\n",
    "    tempSeries = pd.Series(courseLink, name=\"tempLink\")\n",
    "    splitTempSeries = tempSeries.str.split(\"/\", expand=True)\n",
    "    splitTempSeries.columns = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\"]\n",
    "    duplicatesDropped = splitTempSeries.drop_duplicates(\"f\", keep=\"first\").reset_index(drop=True)\n",
    "    courseLink = duplicatesDropped.agg(\"/\".join, axis=1) # This returns a series of course links\n",
    "    print(f\"Total {courseLink.shape[0]} Courses will be Scraped.\")\n",
    "    return courseLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to scrape course links\n",
    "courseLink = main1(\"https://www.reed.co.uk/courses/discount\")\n",
    "\n",
    "# Save course link as xlsx\n",
    "pd.DataFrame({\"courseLink\":courseLink}).to_excel(f\"courseLink_{today}.xlsx\", index=None)\n",
    "\n",
    "# Read course links from the current directory\n",
    "courseLink = pd.concat([pd.read_excel(f) for f in glob.glob(f\"courseLink_{today}.xlsx\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use function main2 to scrape course info by chunks (5)\n",
    "def main2(chunkNo, chunkSize=5):\n",
    "    \"\"\"chunkNo = Index of the chunk to scrape,\n",
    "    chunkSize = No. of chunks the course link will be splitted into (default=5)\"\"\"\n",
    "    \n",
    "    # Split course links into 5 chunks\n",
    "    courseLinkSplitted = np.array_split(courseLink.courseLink.to_list(), chunkSize)\n",
    "    \n",
    "    # Scrapes course info from course link\n",
    "    with ProcessPoolExecutor() as ex:\n",
    "        courseInfo = pd.concat(list(ex.map(scrapeCourseInfo, courseLinkSplitted[chunkNo]))).reset_index(drop=True)\n",
    "    \n",
    "    # Cleans and engineers new features from the scraped dataframe and returns the final dataframe   \n",
    "    finalDf = cleanAndExtractFeature(courseInfo).reset_index(drop=True)\n",
    "    \n",
    "    # Prints how many records scraped\n",
    "    print(f\"Finished! Scraped {finalDf.shape[0]} courses.\")\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scrape by chunks and save as excel\n",
    "chunk0 = main2(0)\n",
    "chunk0.to_excel(f\"chunk0_{today}.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scrape by chunks\n",
    "chunk1 = main2(1)\n",
    "chunk1.to_excel(f\"chunk1_{today}.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scrape by chunks\n",
    "chunk2 = main2(2)\n",
    "chunk2.to_excel(f\"chunk2_{today}.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scrape by chunks\n",
    "chunk3 = main2(3)\n",
    "chunk3.to_excel(f\"chunk3_{today}.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scrape by chunks\n",
    "chunk4 = main2(4)\n",
    "chunk4.to_excel(f\"chunk4_{today}.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>courseId</th>\n",
       "      <th>courseTitle</th>\n",
       "      <th>courseLink</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>courseProvider</th>\n",
       "      <th>offerPrice</th>\n",
       "      <th>originalPrice</th>\n",
       "      <th>unitSold</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>awardingBody</th>\n",
       "      <th>qualName</th>\n",
       "      <th>isRegulated</th>\n",
       "      <th>hasProfCert</th>\n",
       "      <th>soldOrEnq</th>\n",
       "      <th>savingsPercent</th>\n",
       "      <th>broadCategory1</th>\n",
       "      <th>broadCategory2</th>\n",
       "      <th>subCategory1</th>\n",
       "      <th>subCategory2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09_Apr_21</td>\n",
       "      <td>162300</td>\n",
       "      <td>Food Hygiene Course - Level 2</td>\n",
       "      <td>https://www.reed.co.uk/courses/food-hygiene-co...</td>\n",
       "      <td>ROSPA Accredited, Institute of Hospitality End...</td>\n",
       "      <td>The Training Terminal</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10075</td>\n",
       "      <td>[['Food science', 'Food safety'], ['Health and...</td>\n",
       "      <td>...</td>\n",
       "      <td>Royal Society for the Prevention of Accidents</td>\n",
       "      <td>Level 2 Award</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>Food science</td>\n",
       "      <td>Hospitality &amp; catering</td>\n",
       "      <td>Food safety</td>\n",
       "      <td>Food hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09_Apr_21</td>\n",
       "      <td>266620</td>\n",
       "      <td>Leadership &amp; Management</td>\n",
       "      <td>https://www.reed.co.uk/courses/leadership-mana...</td>\n",
       "      <td>Level 7 Advanced Diploma | 150 CPD Points |*FR...</td>\n",
       "      <td>CPD Courses</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>8942</td>\n",
       "      <td>[['HR', 'Leadership'], ['Management', 'Leaders...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Quality Licence Scheme</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>HR</td>\n",
       "      <td>Management</td>\n",
       "      <td>Leadership</td>\n",
       "      <td>Leadership &amp; management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09_Apr_21</td>\n",
       "      <td>227846</td>\n",
       "      <td>Microsoft Excel</td>\n",
       "      <td>https://www.reed.co.uk/courses/microsoft-excel...</td>\n",
       "      <td>Spring Sale! | FREE PDF Certificate | CPD Cert...</td>\n",
       "      <td>Janets</td>\n",
       "      <td>10.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>1758</td>\n",
       "      <td>[['Office skills', 'Microsoft Office', 'Micros...</td>\n",
       "      <td>...</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>Office skills</td>\n",
       "      <td>Business</td>\n",
       "      <td>Microsoft Excel</td>\n",
       "      <td>Microsoft Excel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09_Apr_21</td>\n",
       "      <td>274566</td>\n",
       "      <td>Food Hygiene and Safety Level 3</td>\n",
       "      <td>https://www.reed.co.uk/courses/food-hygiene-an...</td>\n",
       "      <td>**Health Day Gifts:Free PDF Certificate+Free H...</td>\n",
       "      <td>Training Express Ltd</td>\n",
       "      <td>15.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>2246</td>\n",
       "      <td>[['Health and safety', 'Food safety'], ['Hospi...</td>\n",
       "      <td>...</td>\n",
       "      <td>Institute of Hospitality</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>Health and safety</td>\n",
       "      <td>Hospitality &amp; catering</td>\n",
       "      <td>Food safety</td>\n",
       "      <td>Food hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09_Apr_21</td>\n",
       "      <td>98276</td>\n",
       "      <td>Project Management</td>\n",
       "      <td>https://www.reed.co.uk/courses/project-managem...</td>\n",
       "      <td>Advanced Diploma QLS Level 7 |*FREE PDF Certif...</td>\n",
       "      <td>Oxford Home Study College</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>5296</td>\n",
       "      <td>[['IT', 'Project management'], ['Project manag...</td>\n",
       "      <td>...</td>\n",
       "      <td>The Quality Licence Scheme</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>IT</td>\n",
       "      <td>Project management</td>\n",
       "      <td>Project management</td>\n",
       "      <td>PMI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  courseId                      courseTitle  \\\n",
       "0  09_Apr_21    162300    Food Hygiene Course - Level 2   \n",
       "1  09_Apr_21    266620          Leadership & Management   \n",
       "2  09_Apr_21    227846                  Microsoft Excel   \n",
       "3  09_Apr_21    274566  Food Hygiene and Safety Level 3   \n",
       "4  09_Apr_21     98276               Project Management   \n",
       "\n",
       "                                          courseLink  \\\n",
       "0  https://www.reed.co.uk/courses/food-hygiene-co...   \n",
       "1  https://www.reed.co.uk/courses/leadership-mana...   \n",
       "2  https://www.reed.co.uk/courses/microsoft-excel...   \n",
       "3  https://www.reed.co.uk/courses/food-hygiene-an...   \n",
       "4  https://www.reed.co.uk/courses/project-managem...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  ROSPA Accredited, Institute of Hospitality End...   \n",
       "1  Level 7 Advanced Diploma | 150 CPD Points |*FR...   \n",
       "2  Spring Sale! | FREE PDF Certificate | CPD Cert...   \n",
       "3  **Health Day Gifts:Free PDF Certificate+Free H...   \n",
       "4  Advanced Diploma QLS Level 7 |*FREE PDF Certif...   \n",
       "\n",
       "              courseProvider offerPrice originalPrice  unitSold  \\\n",
       "0      The Training Terminal       10.0          30.0     10075   \n",
       "1                CPD Courses       10.0        1050.0      8942   \n",
       "2                     Janets       10.0         949.0      1758   \n",
       "3       Training Express Ltd       15.0         299.0      2246   \n",
       "4  Oxford Home Study College       10.0        1050.0      5296   \n",
       "\n",
       "                                            category  ...  \\\n",
       "0  [['Food science', 'Food safety'], ['Health and...  ...   \n",
       "1  [['HR', 'Leadership'], ['Management', 'Leaders...  ...   \n",
       "2  [['Office skills', 'Microsoft Office', 'Micros...  ...   \n",
       "3  [['Health and safety', 'Food safety'], ['Hospi...  ...   \n",
       "4  [['IT', 'Project management'], ['Project manag...  ...   \n",
       "\n",
       "                                    awardingBody       qualName isRegulated  \\\n",
       "0  Royal Society for the Prevention of Accidents  Level 2 Award           0   \n",
       "1                     The Quality Licence Scheme             na           0   \n",
       "2                                             na             na           0   \n",
       "3                       Institute of Hospitality             na           0   \n",
       "4                     The Quality Licence Scheme             na           0   \n",
       "\n",
       "  hasProfCert soldOrEnq  savingsPercent     broadCategory1  \\\n",
       "0           0         1              66       Food science   \n",
       "1           0         1              99                 HR   \n",
       "2           0         1              98      Office skills   \n",
       "3           0         1              94  Health and safety   \n",
       "4           0         1              99                 IT   \n",
       "\n",
       "           broadCategory2        subCategory1             subCategory2  \n",
       "0  Hospitality & catering         Food safety             Food hygiene  \n",
       "1              Management          Leadership  Leadership & management  \n",
       "2                Business     Microsoft Excel          Microsoft Excel  \n",
       "3  Hospitality & catering         Food safety             Food hygiene  \n",
       "4      Project management  Project management                      PMI  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read today's all the chunk files from the current dir\n",
    "masterDf = pd.concat([pd.read_excel(f) for f in sorted(glob.glob(f\"chunk*_{today}.xlsx\"))]).reset_index(drop=True)\n",
    "masterDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38219, 23), 209)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No of courses been scraped with total providers\n",
    "masterDf.shape, masterDf.courseProvider.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as csv\n",
    "masterDf.to_csv(f\"/home/faysal/Desktop/masterData/allDiscountCourse/{today}_allDiscount.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the directories by removing file not required any more\n",
    "chunksToRemove = glob.glob(f\"chunk*_{today}.xlsx\")\n",
    "linkToRemove = glob.glob(f\"courseLink*_{today}.xlsx\")\n",
    "\n",
    "# Clean the directory. Remove links and chunks\n",
    "[os.remove(f) for f in chunksToRemove]\n",
    "[os.remove(f) for f in linkToRemove]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
