{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b2da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import chain\n",
    "import time\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af29140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awarding bodies first page\n",
    "awarding_body = {\n",
    "    \"skill_for_care\":\"https://www.reed.co.uk/courses/?pageno=1&sortby=MostPopular&pagesize=100&awardingbody=Skills%20for%20Care\",\n",
    "    \"city_and_guilds\":\"https://www.reed.co.uk/courses/?pageno=1&keywords=city%20and%20guilds&sortby=MostPopular&pagesize=100\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36164add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Define deaders and date\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"}\n",
    "today = pd.to_datetime(\"today\").strftime(\"%d_%b_%y\")\n",
    "\n",
    "\n",
    "# Generate cover pages links\n",
    "def generateCoverPageLink(url):\n",
    "    \"\"\"This function generates cover page links from total courses,\n",
    "    url = url to make request in which total course number is found,\n",
    "    return = cover pages links\"\"\"\n",
    "    \n",
    "    # Store cover page links\n",
    "    coverPageLink = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape total course number\n",
    "    totalCourse = int(s.find(\"span\", class_=\"h1\").text.strip().replace(\",\", \"\"))\n",
    "    \n",
    "    # Create stop page\n",
    "    stopPage = int(np.ceil(totalCourse/100))\n",
    "    \n",
    "    # Iterate through stop page and create cover page links\n",
    "    for page in range(1, stopPage+1):\n",
    "        coverPageLink.append(url.split(\"pageno=1\")[0] + f\"pageno={page}\" + url.split(\"pageno=1\")[-1])\n",
    "    return coverPageLink\n",
    "\n",
    "\n",
    "# This function scrapes individual course links from every cover page links\n",
    "def scrapeIndividualCourseLink(url):\n",
    "    \"\"\"Scrapes individual course link from cover page links,\n",
    "    url = cover page link,\n",
    "    return = Individual course links\"\"\"\n",
    "    \n",
    "    # Store course links\n",
    "    courseLink = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape course links and store\n",
    "    for lnk in s.find_all(\"div\", class_=\"course-overview\"):\n",
    "        courseLink.append(\"https://www.reed.co.uk\" + lnk.find(\"h2\").find(\"a\").get(\"href\"))\n",
    "    return courseLink\n",
    "\n",
    "\n",
    "# This function scrapes individual course info from individual course link\n",
    "def scrapeCourseInfo(url):\n",
    "    \"\"\"url = individual course link,\n",
    "    return = scraped course info as a dataframe\"\"\"\n",
    "    \n",
    "    # Initialize empty list of variables to be scraped\n",
    "    courseTitle = []\n",
    "    subtitle = []\n",
    "    offerPrice = []\n",
    "    originalPrice = []\n",
    "    courseProvider = []\n",
    "    unitSold = []\n",
    "    category = []\n",
    "    savings = []\n",
    "    haveCpd = []\n",
    "    awardingBody = []\n",
    "    qualName = []\n",
    "    cpdPoint = []\n",
    "    isRegulated = []\n",
    "    hasProfCert = []\n",
    "    soldOrEnq = []\n",
    "    \n",
    "    # Making request\n",
    "    r = requests.get(url, headers=HEADERS, verify=False)\n",
    "    s = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Scrape course name\n",
    "    try:\n",
    "        courseTitle.append(s.find(\"div\", class_=\"course-title\").find(\"h1\").text.strip())\n",
    "    except:\n",
    "        courseTitle.append(\"na\")\n",
    "        \n",
    "    # Scrape subtitle\n",
    "    try:\n",
    "        subtitle.append(s.find(\"div\", class_=\"course-title\").find(\"h2\").text.strip())\n",
    "    except:\n",
    "        subtitle.append(\"na\")\n",
    "        \n",
    "    # Scrape offer price\n",
    "    try:\n",
    "        offerPrice.append(s.find(\"span\", class_=\"current-price\").text.strip())\n",
    "    except:\n",
    "        offerPrice.append(\"na\")\n",
    "        \n",
    "    # Scrape original price\n",
    "    try:\n",
    "        originalPrice.append(s.find(\"small\", class_=\"vat-status\").text.strip())\n",
    "    except:\n",
    "        originalPrice.append(\"na\")\n",
    "        \n",
    "    # Scrape course provider\n",
    "    try:\n",
    "        try:\n",
    "            courseProvider.append(s.find(\"a\", class_=\"provider-link\").text.strip())\n",
    "        except:\n",
    "            courseProvider.append(s.find(\"span\", class_=\"thumbnail\").text.strip())\n",
    "    except:\n",
    "        courseProvider.append(\"na\")\n",
    "            \n",
    "    # Scrape unit sale\n",
    "    try:\n",
    "        unitSold.append(s.find(id=\"number-enquiries-purchases\").text.strip())\n",
    "    except:\n",
    "        unitSold.append(0)\n",
    "        \n",
    "    # Scrape category\n",
    "    try:\n",
    "        # Scrape total category\n",
    "        totalCat = len(s.find_all(\"ol\", class_=\"breadcrumb pb-0\"))\n",
    "        for cat in range(totalCat):\n",
    "            category.append([x.text.strip() for x in s.find_all(\"ol\", class_=\"breadcrumb pb-0\")[cat].find_all(\"li\")])\n",
    "    except:\n",
    "        category.append(\"na\")\n",
    "        \n",
    "    # Scrape savings\n",
    "    try:\n",
    "        savings.append(s.find(\"span\", class_=\"icon-savings-tag price-saving\").text.strip())\n",
    "    except:\n",
    "        savings.append(\"na\")\n",
    "        \n",
    "    # Does the course have CPD?\n",
    "    try:\n",
    "        haveCpd.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-cpd mt-2\") else 0)\n",
    "    except:\n",
    "        haveCpd.append(0)\n",
    "    \n",
    "    # Scrape awarding body\n",
    "    try:\n",
    "        try:\n",
    "            # Executes if the course is \"Endorsed by\"\n",
    "            awardingBody.append(s.find(\"div\", class_=\"col\").find(\"a\").text.strip())\n",
    "        except:\n",
    "            # Executes if the course is \"Awarded by\"\n",
    "            awardingBody.append(s.find(\"div\", class_=\"small\").find(\"div\").find(\"a\").text.strip())\n",
    "    except:\n",
    "        awardingBody.append(\"na\")\n",
    "            \n",
    "    # Scrape qualification name\n",
    "    try:\n",
    "        qualName.append(s.find(\"div\", class_=\"small\").find(\"h3\", class_=\"h4\").text.strip())\n",
    "    except:\n",
    "        qualName.append(\"na\")\n",
    "    \n",
    "    # Scrape cpd point\n",
    "    try:\n",
    "        cpdPoint.append(s.body.find_all(text=re.compile(\"\\d{1,3}\\sCPD hours / points\"))[0].strip())\n",
    "    except:\n",
    "        cpdPoint.append(0)\n",
    "        \n",
    "    # Is the course regulated? Assign 1 if regulated, otherwise 0\n",
    "    try:\n",
    "        isRegulated.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-regulated mt-2\") else 0)\n",
    "    except:\n",
    "        isRegulated.append(0)\n",
    "        \n",
    "    # Does the course offer professional certification?\n",
    "    try:\n",
    "        hasProfCert.append(1 if s.find(\"div\", class_=\"badge badge-dark badge-professional mt-2\") else 0)\n",
    "    except:\n",
    "        hasProfCert.append(0)\n",
    "    \n",
    "    # Check if the course is sold or enquired. 2 if the course has both purchade and enquired mode\n",
    "    try:\n",
    "        soldOrEnq.append(2 if (s.find(id=\"addToBasket\") and s.find(id=\"enquireNow\"))\\\n",
    "        else 1 if s.find(id=\"addToBasket\") else 0)\n",
    "    except:\n",
    "        soldOrEnq.append(\"na\")\n",
    "        \n",
    "    # Create a df off scraped variables\n",
    "    df = pd.DataFrame({\n",
    "        \"courseTitle\":courseTitle,\n",
    "        \"courseLink\":url,\n",
    "        \"subtitle\":subtitle,\n",
    "        \"courseProvider\":courseProvider,\n",
    "        \"offerPrice\":offerPrice,\n",
    "        \"originalPrice\":originalPrice,\n",
    "        \"unitSold\":unitSold,\n",
    "        \"category\":[category], # This one is not scalar, converting into 1d\n",
    "        \"haveCpd\":haveCpd,\n",
    "        \"cpdPoint\":cpdPoint,\n",
    "        \"awardingBody\":[awardingBody],\n",
    "        \"qualName\":qualName,\n",
    "        \"isRegulated\":isRegulated,\n",
    "        \"hasProfCert\":hasProfCert,\n",
    "        \"savings\":savings,\n",
    "        \"soldOrEnq\":soldOrEnq\n",
    "    })\n",
    "    df = df.astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# This function cleans scraped data.\n",
    "def cleanAndExtractFeature(df):\n",
    "    \"\"\"\"df = dataFrame to clean,\n",
    "    return = final cleaned data\"\"\"\n",
    "    \n",
    "    # Copy the input data\n",
    "    finalDf = df.copy()\n",
    "    \n",
    "    # Create course id and insert to the df\n",
    "    finalDf.insert(loc=0, value=finalDf.courseLink.str.split(\"/\").str.get(5).str.replace(\"#\",\"\"), \n",
    "                     column=\"courseId\")\n",
    "    \n",
    "    # Insert date\n",
    "    finalDf.insert(loc=0, value=today, column=\"date\")\n",
    "    \n",
    "    # Clean unit sold\n",
    "    finalDf.unitSold = finalDf.unitSold.apply(lambda x: re.findall(r\"\\d+\", x)).str.join(\"\")\n",
    "    finalDf.unitSold = pd.to_numeric(finalDf.unitSold, errors=\"coerce\").fillna(0).astype(\"int\")\n",
    "    \n",
    "    # Clean saving percent\n",
    "    finalDf[\"savingsPercent\"] = finalDf.savings.str.split(\"Save\").str[-1].str.replace(\"%\", \"\")\n",
    "    finalDf.savingsPercent = pd.to_numeric(finalDf.savingsPercent, errors=\"coerce\").fillna(0).astype(\"int\")\n",
    "    \n",
    "    # Clean offer price\n",
    "    finalDf.offerPrice = finalDf.offerPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\")\n",
    "    \n",
    "    # Clean original price\n",
    "    finalDf.originalPrice = finalDf.originalPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\").str.replace(\")\", \"\")\n",
    "    finalDf.originalPrice = pd.to_numeric(finalDf.originalPrice, errors=\"coerce\")\n",
    "    \n",
    "    # If savings is 0, make offer price equals to original price\n",
    "    finalDf.originalPrice = np.where(finalDf.savingsPercent==0,\n",
    "                                     finalDf.originalPrice.fillna(finalDf.offerPrice), finalDf.originalPrice)\n",
    "    \n",
    "    # Extract CPD point\n",
    "    finalDf[\"cpdPoint\"] = pd.to_numeric(finalDf.cpdPoint.str.join(\"\").str.split(\"CPD\").str[0], errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    \n",
    "    # Remove '\"' from category\n",
    "    finalDf.category = finalDf.category.apply(lambda x: eval(x))\n",
    "    \n",
    "    # Create broadCategory1 from category\n",
    "    finalDf[\"broadCategory1\"] = finalDf.category.str[0].str[0]\n",
    "    \n",
    "    # Create broadCategory2 from category\n",
    "    finalDf[\"broadCategory2\"] = finalDf.category.str[-1].str[0]\n",
    "    \n",
    "    # Create subCategory1 from category\n",
    "    finalDf[\"subCategory1\"] = finalDf.category.str[0].str[-1]\n",
    "    \n",
    "    # Create subCategory1 from category\n",
    "    finalDf[\"subCategory2\"] = finalDf.category.str[-1].str[-1]\n",
    "    \n",
    "    # Extract qualification name\n",
    "    finalDf[\"qualName\"] = np.where(finalDf.qualName.str.contains(\"CPD\"), \"na\", finalDf.qualName)\n",
    "    \n",
    "    # Clean awarding body\n",
    "    finalDf.awardingBody = finalDf.awardingBody.str[1:-1].str[1:-1]\\\n",
    "    .str.strip().replace(r\"^\\s*$\", np.nan, regex=True).fillna(\"na\")\n",
    "    \n",
    "    # Drop \"savings\"\n",
    "    finalDf.drop(\"savings\", axis=1, inplace=True)\n",
    "    \n",
    "    # Drop duplicates by \"courseId\"\n",
    "    finalDf = finalDf.drop_duplicates(\"courseId\")\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcc0e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all the functions inside main\n",
    "def main(url):\n",
    "    \"\"\"url = url to make the 1st requests to generate cover pages,\n",
    "    return = final cleaned dataframe\"\"\"\n",
    "    \n",
    "    # Record start time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Generates cover pages links\n",
    "    coverPageLink = generateCoverPageLink(url)\n",
    "    \n",
    "    # Store individual course links\n",
    "    courseLink = []\n",
    "    \n",
    "    # This loop ensures maximum no of course links scraped\n",
    "    for _ in range(4):\n",
    "        with ProcessPoolExecutor(max_workers=6) as ex:\n",
    "            # Scrape individual course links\n",
    "            indCourseLink = list(ex.map(scrapeIndividualCourseLink, coverPageLink))\n",
    "            indCourseLink = list(chain(*indCourseLink)) # Flattening the list\n",
    "        courseLink.append(indCourseLink)\n",
    "    courseLink = list(chain(*courseLink))\n",
    "    \n",
    "    # Create a series to drop duplicates by ids. This portion keeps only the unique links\n",
    "    tempSeries = pd.Series(courseLink, name=\"tempLink\")\n",
    "    splitTempSeries = tempSeries.str.split(\"/\", expand=True)\n",
    "    splitTempSeries.columns = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\"]\n",
    "    duplicatesDropped = splitTempSeries.drop_duplicates(\"f\", keep=\"first\").reset_index(drop=True)\n",
    "    courseLink = duplicatesDropped.agg(\"/\".join, axis=1) # This returns a series of course links\n",
    "    \n",
    "        \n",
    "    # Scrapes course info from course link\n",
    "    with ProcessPoolExecutor(max_workers=6) as ex:\n",
    "        courseInfo = pd.concat(list(ex.map(scrapeCourseInfo, courseLink))).reset_index(drop=True)\n",
    "    \n",
    "    # Cleans and engineers new features from the scraped dataframe and returns the final dataframe   \n",
    "    finalDf = cleanAndExtractFeature(courseInfo).reset_index(drop=True)\n",
    "    \n",
    "    # Measure execution time and return the final df\n",
    "    endTime = time.time()\n",
    "    durationInMins = round((endTime-startTime)/60, 2)\n",
    "    print(f\"{len(courseLink)} Records ==> {durationInMins} Minutes\")\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b542f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 Records ==> 0.75 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-5fd7b4cc34bb>:232: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  finalDf.originalPrice = finalDf.originalPrice.str.split(\"£\").str[-1].str.replace(\",\", \"\").str.replace(\")\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# Skill for care\n",
    "skill_for_care = main(awarding_body[\"skill_for_care\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e961a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_for_care.to_excel(\"skill_for_care_20_sep.xlsx\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
